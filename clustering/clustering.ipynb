{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Tutorial Activity\n",
    "\n",
    "Clustering is a broad set of *unsupervised learning* techniques used to find subgroups within a dataset. The goal of the clustering algorithm is to split up the data so that points within a single cluster are very similar and points in different clusters are different. We can ask questions such as \"Is there an informative way to visualize the data?\" or \"Can we discover subgroups among the variables or among the observations?\". The most popular clustering methods include *k*-Means clustering and hierarchical clustering. *K*-Means clustering seeks to partition the observations into pre-specified number of clusters. Hierarchical clustering is a bottom-up approach seeking to build a hierarchy of clusters\n",
    "\n",
    "After completing this tutorial you should be able to:\n",
    "- conceptually understand the k-means clustering algorithm <br/>\n",
    "- use `sci-kit learn` to perform k-means clustering and hierarchical clustering <br/>\n",
    "- use `matplotlib` visualize the results from k-means clustering and hierarchical clustering\n",
    "- make justified decisions on how many optimal clusters exist in the dataset\n",
    "- understand the impact of noisy data on clustering algorithms\n",
    "- understand the benefits and limitations to modeling data after reducing features by clustering\n",
    "\n",
    "\n",
    "## Further reading\n",
    " 1. Github is a great resource! Introduction to Machine Learning with Python by Andreas Muller & Sarah Guido can be found [here](https://github.com/amueller/introduction_to_ml_with_python).\n",
    " 2. Other readings that the MSU Machine Learning Group has used can be found [here](https://drive.google.com/drive/u/0/folders/1F2hcSpIa_jWyVCVS51oEO1dXzWP_kOu2).\n",
    "\n",
    "## Data\n",
    "In the data folder, you will find a few different data sets. To conceptually understand the cluster algorithms you will use the `clustering_data.csv`. The clustering data is a simulated dataset of education-like samples. From college GPA and the percentage of passed courses you will attempt to cluster students on whether they get a degree or not. This is encoded in the `degree` column of the dataset as a `1` for those who get a degree and `0` otherwise. The \"Challenges\" at the end of the tutorial will utilize th `clustering_data.csv`dataset.\n",
    "\n",
    "## K-Means Clustering\n",
    "\n",
    "*K*-means clustering is a simple approach for partitioning a data set into *K* distinct, non-overlappying clusters. To perform *k*-means clustering, the researcher must specify the desired number of clusters before running the algorithm. There are alternative approaches to clustering which does not require that you commit to a particular number of clusters. We will explore this later but for now, let's focus on *k*-means clustering.\n",
    "\n",
    "### Task 1: Scaling the data\n",
    "The *K*-means algorithm is centroid based, which means it work off measuring distances between datapoints. As such the scale of the features are very important such that small variations in a variable with a large magnitude does not overpower larger variations in features of smaller scale. \n",
    "1. Import the `sample_algorithm.csv` file and visualize the data using `plt.plot` used in the previous tutorials. This dataset has only 2 features so a simple 2-D plot will work. Does this dataset appear to have a clustering structure? If so, how many clusters do you think are in the dataset?\n",
    "2. Does the data exist on different scales? If so rescale the data to a uniform interval using the z-scaling function $f(x) = \\frac{x - \\bar{x}}{\\sigma}$ where $\\bar{x}$ is the sample mean and $\\sigma$ the sample standard deviation \n",
    "\n",
    "### Task 2: Conceptually understanding the *k*-means clustering algorithm\n",
    "\n",
    "1. It looks as though there might be 2 clusters in the data. Choose two random points in the x-y plane to be used as the initial centers. Plot them on the previous graph as an 'X'. *Note: Each center needs to be initialized at differnt point in space.*\n",
    "2. Define three new \"distance\" columns in the dataset to calculate the distances between each of the three centroids and each observation. The most common distance metric used in a clustering analysis is the Euclidean distance. Below is a function already written for you to use.\n",
    "```python\n",
    "def calculate_distance(initial, X, Y):\n",
    "    c_x, c_y = initial\n",
    "    root_diff_x = (X - c_x) ** 2\n",
    "    root_diff_y = (Y - c_y) ** 2\n",
    "    distance = np.sqrt(root_diff_x + root_diff_y)\n",
    "    return distance\n",
    "```\n",
    "4. For each observation, compare the three distances and chose the *smallest* one (use [`np.argmin`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmin.html).) Using the `map` function, label the centroids accordingly in a new column called \"Clusters\".\n",
    "5. Find the new centroid points by taking the means for both features of each of the three clusters. Make a new plot of the data, coloring the three clusters and labeling the new centroids as 'D'. What happened to the three centroids as a result of this algorithm?\n",
    "6. Make the new centroid points as the 'initial' centroids. Repeat steps 3-5 for a second iteration. What happened to the three centroids after this second iteration? When do we know the algorithm is complete and we have our defined clusters?\n",
    "\n",
    "### Task 2: Oh wait.... `sci-kit learn` has a *k*-means function to make things easier!!\n",
    "1. Because `sci-kit learn` has a built in function for clustering so you no long have to do this by hand! For this, you will need to import the following: \n",
    "````python\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "````\n",
    "2. Import the `clustering_data.csv` file and visualize the data using `plt.plot` used in the previous tutorials. Since this dataset has more than 2 features, you may want to use a scatterplot matrix that you've used previously. Does this dataset appear to have a clustering structure? If so, how many clusters do you think are in the dataset?\n",
    "3. Prior to using the `KMeans` function, you need to clean the data to only include the \"features\" columns. Wait...how will the computer calculate a Euclidean distance with a dataset that has 5 features?\n",
    "4. First, we need to reduce our dimensions using whats called Principle Component Analysis (PCA). Here, use the `PCA.fit_transform` function to transform the data to 2 dimensions. Use [this](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit_transform) as a resource. \n",
    "5. Now that we have the data in 2-D, we can easily perform a *k*-means clustering on the reduced data. Use [this](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) resource to perform a *k*-means, predict the clusters, find the cluster centers, and plot the results. Below is a plotting definition to help you:\n",
    "````python\n",
    "def kmeans_plot(data,label,centers):\n",
    "    \n",
    "    x_min, x_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    y_min, y_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    \n",
    "    plt.scatter(data[:, 1], data[:, 0], c=label, cmap = 'viridis')\n",
    "    plt.scatter(centers[:, 1], centers[:, 0], marker='x', c = 'k')\n",
    "    \n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    title = \"KMeans Clustering of a PCA transformed dataset with \"+str(len(centers))+\" clusters\"\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "````\n",
    "6. What happens if you change the number of clusters? Can you justify the optimal number of clusters in the dataset?\n",
    "\n",
    "## Hierarchical Clustering\n",
    "A big limitation to *k*-means clustering is that the researcher has to provide the algorithm with the number of clusters. But what if the data doesn't show a clear cluster structure and therefore, the researcher has no idea how many clusters would make sense? Hierarchical clustering is a bottom-up approach that seeks to build a hierarchy of clusters. Hierarchical clustering produces what's called a dendrogram which can be analyzed in order to decide how many clusters exist in the dataset. Remember the overall goal of clustering is to split up the data so that points within a single cluster are very similar and points in different clusters are different. A dendrogram plots the distances between two points and so looking for when the distance between clusters in the greatest can determine how many clusters exist in the dataset. Let's explore...\n",
    "\n",
    "### Task 3: Perform the Hierarchical Clustering using `scipy.cluster.hierarchy`\n",
    "1. Continue to use the `clustering_data.csv` dataset. For hierarchical clustering, you will need to import the following: \n",
    "````python\n",
    "import scipy.cluster.hierarchy as shc\n",
    "````\n",
    "2. Perform a hierarchical clustering:\n",
    " - First, you will need to decide what \"linkage\" you will use. For our purposes, let's us \"Ward\" method ([`shc.linkage`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html)). \n",
    " - Next, visualize the data ([`shc.dendrogram`](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html)).\n",
    "3. Add a horizontal line at y = 300. How many clusters are there?\n",
    "4. Add a horizontal line at y = 175. How many clusters are there?\n",
    "5. Add a horizontal line at y = 125. How many clusters are there?\n",
    "6. Does the hierarchical clustering analysis change the decisions you made in the prior *k*-means analysis you did earlier?\n",
    "\n",
    "## Challenges\n",
    "\n",
    "After you've mastered the above two clustering algorithms, you should be able to explore the practical usage of clustering in more detail.\n",
    "1. Using the `clustering_data` dataset, how does reducing the number of total features impact the model fit in the outcome variable provided.\n",
    "2. Using `sci-kit learn`, how does the level of noise in the data impact how \"easy\" clustering is? Hint: You will need to use the following to produce your own clustering dataset:\n",
    "````python\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
