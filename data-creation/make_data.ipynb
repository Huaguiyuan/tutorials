{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_moons, make_regression, make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score, explained_variance_score\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_curve, roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# clustering\n",
    "# from sklearn.datasets import make_blobs, make_moons\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "\n",
    "# from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# data exploration\n",
    "# from sklearn.datasets import make_regression, make_moons\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.neural_network import MLPRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# from sklearn.metrics import r2_score, explained_variance_score\n",
    "\n",
    "# from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# classification\n",
    "# from sklearn.datasets import make_classification, make_moons\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from sklearn.metrics import f1_score, roc_curve, roc_auc_score\n",
    "\n",
    "# from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# regression\n",
    "\n",
    "# from sklearn.datasets import make_regression, make_moons\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.neural_network import MLPRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# from sklearn.metrics import r2_score, explained_variance_score\n",
    "\n",
    "# from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval_transform(x, a, b):\n",
    "    m = x.min()\n",
    "    ma = x.max()\n",
    "    \n",
    "    alpha_inv = (1 - m/ma)*ma/(a - b)\n",
    "    alpha = 1/alpha_inv\n",
    "    beta = b - alpha*m\n",
    "    \n",
    "    f = lambda x: alpha*x + beta \n",
    "    return f(x)\n",
    "\n",
    "def make_noise_feature(x):\n",
    "    n_features = x.shape[1]\n",
    "    n_samples = x.shape[0]\n",
    "    \n",
    "    weights = np.random.uniform(1e-4, 1e-2, n_features)\n",
    "    noise = np.random.normal(1, 5, n_samples)\n",
    "    signal = np.sum(weights*x, -1) \n",
    "    return signal + noise\n",
    "\n",
    "def calculate_pvalues(df,\n",
    "                      method = spearmanr\n",
    "                     ):\n",
    "    \"\"\"\n",
    "    Assumes df with only numeric entries clean of null entries. \n",
    "    \"\"\"\n",
    "    dfcols = pd.DataFrame(columns=df.columns)\n",
    "    pvalues = dfcols.transpose().join(dfcols, how='outer')\n",
    "    for r in df.columns:\n",
    "        for c in df.columns:\n",
    "            pvalues[r][c] = round(method(df[r], df[c])[1], 4)\n",
    "    return pvalues\n",
    "\n",
    "def correlation_matrix(df,\n",
    "                       method = \"pearson\",\n",
    "                       annot_bool = False,\n",
    "                       annot_size = 20\n",
    "                      ):\n",
    "    # Compute the correlation matrix\n",
    "    corr = df.corr(method = method)\n",
    "\n",
    "    if annot_bool:\n",
    "        annot = corr.copy()\n",
    "        \n",
    "        if method == \"pearson\":\n",
    "            sig_meth = pearsonr\n",
    "        else:\n",
    "            sig_meth = spearmanr\n",
    "            \n",
    "        pval = calculate_pvalues(df, sig_meth) \n",
    "        # create three masks\n",
    "        r0 = corr.applymap(lambda x: '{:.2f}'.format(x))\n",
    "        r1 = corr.applymap(lambda x: '{:.2f}*'.format(x))\n",
    "        r2 = corr.applymap(lambda x: '{:.2f}**'.format(x))\n",
    "        r3 = corr.applymap(lambda x: '{:.2f}***'.format(x))\n",
    "  \n",
    "        # apply them where appropriate --this could be a single liner\n",
    "        annot = annot.where(pval>0.1,r0)\n",
    "        annot = annot.where(pval<=0.1,r1)\n",
    "        annot = annot.where(pval<=0.05,r2)\n",
    "        annot = annot.mask(pval<=0.01,r3)\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 11))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5},\n",
    "                annot = annot,\n",
    "                fmt = \"\",\n",
    "                annot_kws={\"size\": annot_size},\n",
    "                vmin = -1,\n",
    "                vmax = 1,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_info = 3\n",
    "n_redu = 0\n",
    "n_samples=2000\n",
    "\n",
    "#making nonlinear decision boundaries requires multiple blob like features \n",
    "X1, y1 = make_blobs(\n",
    "        n_samples=n_samples,\n",
    "        n_features=2,\n",
    "        centers=np.array([[42, 39], [39.5, 38.3]]),\n",
    "        shuffle=False,\n",
    "        random_state=42, \n",
    "        #difficulty,\n",
    "        cluster_std=1.4,\n",
    "        )\n",
    "\n",
    "X2, y2 = make_blobs(\n",
    "        n_samples=n_samples,\n",
    "        n_features=2,\n",
    "        centers=np.array([[44, 39.8], [38, 37.9]]),\n",
    "        cluster_std=1.2,\n",
    "        shuffle=False,\n",
    "        random_state=6, \n",
    "        #difficulty,\n",
    "    \n",
    "        )\n",
    "\n",
    "X3, y3 = make_moons(n_samples=2*n_samples, noise=1, random_state=42)\n",
    "\n",
    "X = np.concatenate([X1, X2], axis=0)\n",
    "y = np.concatenate([y1, y2], axis=0)\n",
    "\n",
    "data = np.concatenate([X, np.expand_dims(y, -1)], -1)\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpa_column = interval_transform(data[0], 1, 4) \n",
    "passed_column = interval_transform(data[1], 0, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = np.concatenate(\n",
    "                [\n",
    "                    np.expand_dims(gpa_column, axis=-1),\n",
    "                    np.expand_dims(passed_column, axis=-1),\n",
    "                    np.expand_dims(y, axis=-1)\n",
    "                ],\n",
    "                axis=1\n",
    ")\n",
    "\n",
    "columns = [              \n",
    "            \"cGPA\",\n",
    "            \"passed_percent\",\n",
    "            \"degree\",\n",
    "        ]\n",
    "\n",
    "df_full = pd.DataFrame(full_data,\n",
    "                       columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_csv('/home/john/research/tutorials/clustering/data/clustering_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/john/research/tutorials/data-creation'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_info = 3\n",
    "n_redu = 0\n",
    "n_samples=2000\n",
    "\n",
    "#making nonlinear decision boundaries requires multiple blob like features \n",
    "X1, y1 = make_regression(\n",
    "        n_samples=n_samples,\n",
    "        n_features=3,\n",
    "        n_informative=n_info,\n",
    "        #n_redundant=n_redu,\n",
    "        shuffle=False,\n",
    "        random_state=42,\n",
    "        #difficulty\n",
    "        effective_rank=2,\n",
    "        noise=0.6,\n",
    "        tail_strength=0.2,\n",
    "        bias=12,\n",
    "        )\n",
    "\n",
    "X2, y2 = make_regression(\n",
    "        n_samples=n_samples,\n",
    "        n_features=3,\n",
    "        n_informative=n_info,\n",
    "        #n_redundant=n_redu,\n",
    "        shuffle=False,\n",
    "        random_state=6, \n",
    "        #difficulty\n",
    "        effective_rank=1,\n",
    "        noise=1.1,\n",
    "        tail_strength=0.3,\n",
    "        bias=10,\n",
    "        )\n",
    "\n",
    "#X3, y3 = make_moons(n_samples=2*n_samples, noise=1, random_state=42)\n",
    "\n",
    "X = np.concatenate([X1, X2], axis=0)\n",
    "y = np.concatenate([y1, y2], axis=0)\n",
    "\n",
    "data = np.concatenate([X, np.expand_dims(y, -1)], -1)\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "attendance_column = interval_transform(data[2], 0, 100)\n",
    "gpa_column = interval_transform(data[1], 1, 4) \n",
    "passed_column = interval_transform(data[0], 0, 100)\n",
    "sex_column = make_noise_feature(X)\n",
    "sex_column = (sex_column > sex_column.mean()).astype(int)\n",
    "hsgpa_column = interval_transform(make_noise_feature(X), 0, 4)\n",
    "ethn_column = make_noise_feature(X)\n",
    "ethn_column = pd.qcut(ethn_column, q=[0, .25, .5, 1], labels=[0, 1, 2])\n",
    "fci_post = interval_transform(y, 0, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = np.concatenate(\n",
    "                [\n",
    "                    np.expand_dims(gpa_column, axis=-1),\n",
    "                    np.expand_dims(attendance_column, axis=-1),\n",
    "                    np.expand_dims(passed_column, axis=-1),\n",
    "                    np.expand_dims(sex_column, axis=-1),\n",
    "                    np.expand_dims(hsgpa_column, axis=-1),\n",
    "                    np.expand_dims(ethn_column, axis=-1),\n",
    "                    np.expand_dims(fci_post, axis=-1)\n",
    "                ],\n",
    "                axis=1\n",
    ")\n",
    "\n",
    "columns = [              \n",
    "           \"cGPA\",\n",
    "           \"attendance\",\n",
    "           \"passed_percent\",\n",
    "           \"sex\",\n",
    "           \"hsGPA\",\n",
    "           \"ethnicity\",\n",
    "           \"fci_post\"]\n",
    "\n",
    "df_full = pd.DataFrame(full_data,\n",
    "                       columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_csv('/home/john/research/tutorials/exploring-data/data/regression_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_info = 3\n",
    "n_redu = 0\n",
    "n_samples=2000\n",
    "\n",
    "#making nonlinear decision boundaries requires multiple blob like features \n",
    "X1, y1 = make_regression(\n",
    "        n_samples=n_samples,\n",
    "        n_features=3,\n",
    "        n_informative=n_info,\n",
    "        #n_redundant=n_redu,\n",
    "        shuffle=False,\n",
    "        random_state=42,\n",
    "        #difficulty\n",
    "        effective_rank=2,\n",
    "        noise=0.6,\n",
    "        tail_strength=0.2,\n",
    "        bias=12,\n",
    "        )\n",
    "\n",
    "X2, y2 = make_regression(\n",
    "        n_samples=n_samples,\n",
    "        n_features=3,\n",
    "        n_informative=n_info,\n",
    "        #n_redundant=n_redu,\n",
    "        shuffle=False,\n",
    "        random_state=6, \n",
    "        #difficulty\n",
    "        effective_rank=1,\n",
    "        noise=1.1,\n",
    "        tail_strength=0.3,\n",
    "        bias=10,\n",
    "        )\n",
    "\n",
    "#X3, y3 = make_moons(n_samples=2*n_samples, noise=1, random_state=42)\n",
    "\n",
    "X = np.concatenate([X1, X2], axis=0)\n",
    "y = np.concatenate([y1, y2], axis=0)\n",
    "\n",
    "data = np.concatenate([X, np.expand_dims(y, -1)], -1)\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "attendance_column = interval_transform(data[2], 0, 100)\n",
    "gpa_column = interval_transform(data[1], 1, 4) \n",
    "passed_column = interval_transform(data[0], 0, 100)\n",
    "sex_column = make_noise_feature(X)\n",
    "sex_column = (sex_column > sex_column.mean()).astype(int)\n",
    "hsgpa_column = interval_transform(make_noise_feature(X), 0, 4)\n",
    "ethn_column = make_noise_feature(X)\n",
    "ethn_column = pd.qcut(ethn_column, q=[0, .25, .5, 1], labels=[0, 1, 2])\n",
    "fci_post = interval_transform(y, 0, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = np.concatenate(\n",
    "                [\n",
    "                    np.expand_dims(gpa_column, axis=-1),\n",
    "                    np.expand_dims(attendance_column, axis=-1),\n",
    "                    np.expand_dims(passed_column, axis=-1),\n",
    "                    np.expand_dims(sex_column, axis=-1),\n",
    "                    np.expand_dims(hsgpa_column, axis=-1),\n",
    "                    np.expand_dims(ethn_column, axis=-1),\n",
    "                    np.expand_dims(fci_post, axis=-1)\n",
    "                ],\n",
    "                axis=1\n",
    ")\n",
    "\n",
    "columns = [              \n",
    "           \"cGPA\",\n",
    "           \"attendance\",\n",
    "           \"passed_percent\",\n",
    "           \"sex\",\n",
    "           \"hsGPA\",\n",
    "           \"ethnicity\",\n",
    "           \"fci_post\"]\n",
    "\n",
    "df_full = pd.DataFrame(full_data,\n",
    "                       columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_csv('/home/john/research/tutorials/regression/data/regression_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
