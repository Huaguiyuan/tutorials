{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification \n",
    "\n",
    "Classification modelling is a branch of machine learning concerned with separating sets of objects.\n",
    "\n",
    "To do this we usually want to predict from a vector $\\mathbf{x}$ what class it belongs to, $y$. The class can be different species of dog, different human faces, the phase of a thermodynamic system to name a few. In this example you'll be working with simulated student data to predict whether they get a pass or fail grade with three different models. \n",
    "\n",
    "Your learning goals for this project is: \n",
    "* Improved understanding of the `scikit-learn` pipeline for modelling, building on the regression tutorial \n",
    "* Imroved understanding of machine learning concepts: generalization, overfitting, model set, feature scale, and class bias\n",
    "* Understanding the difference between linear and non-linear models \n",
    "* Understanding some important classification measurements: Area under Curve, Reciever Operator Characteristic, Precision, Recall \n",
    "* Understanding the goal of classification modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading: \n",
    "\n",
    "\n",
    "1. A high-bias, low-variance introduction to machine learning, a good introduction for physicists to machine learning concepts https://arxiv.org/abs/1803.08823\n",
    "2. The Elements of Statistical Learning, a very good textbook giving an introduction to much of the theory used in machine learning https://web.stanford.edu/~hastie/Papers/ESLII.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying failure in a class\n",
    "\n",
    "In this project the aim is to predict whether a studen fails or passes a course. The data has the following columns:\n",
    "\n",
    "*         \"cGPA\": college GPA \n",
    "*        \"attendance\": attendance percent\n",
    "*       \"passed_percent\": percent of courses passed\n",
    "*      \"sex\": self reported sex of the student\n",
    "*     \"hsGPA\": high school GPA\n",
    "*    \"ethnicity\": self reported ethnic identity of the student\n",
    "*   \"failed_course\": whether the student failed the course (1) or not (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data is included in the `data` folder in a python pickle file-format. Pandas has a function to read this data into a data-frame; `pd.read_csv(file_name)`. If you prefer not to load files from an untrusted source the data can be generated from the associated notebook. \n",
    "\n",
    "#### filename: `classification_data.csv`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1a:  Exploration\n",
    "Prior to any modelling it is important to understand the type of data you're working with. Many models behave differently under differently scaled features, as well explore later in this project. And some models have different implications for interpretability under multicolinearity in the features. Logistic Regression (LR), perhaps the most simple classifying algorithm, is one such. In LR the coefficients can be interpreted as the change in log odds for a given feature. What does this mean for a feature which can be expressed as a linear combination of others in the same system? As such the use of data exploration, and preprocessing tools like principal component analysis etc. is vital both for performance and in some cases completely necessary for interpretability of the model.  \n",
    "\n",
    "In this task you should:\n",
    "1. visualize all the features and their distributions. \n",
    "2. Like in the regression task you should also investigate the covariance matrix. Will covariance have a different impact for classification than for regression? What models will it impact? \n",
    "3. Visualize the class distribution the dependent variable: `failed_course`. What challenges does this distribution pose, if any? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1b: Scaling\n",
    "A major part of data analysis and modelling is being aware of the impacts of the distribtions of values in the features on a given model. As an example consider a neural network with a sigmoid activation, values above or below ~4 asymptotically approach 1 or -1 respectively. A feature distributed as $x $~$\\mathcal{N}(\\mu=100, \\sigma=2) $ for example will then have the same activation for all values of x, which leads to the model being unable to learn anything from that feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def logistic(x): return 1/(1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-6, 6)\n",
    "y = logistic(x)\n",
    "\n",
    "plt.plot(x, y, label=r\"$f(x) = \\frac{1}{1+e^{-x}}$\", linewidth=2)\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"arb. units\")\n",
    "plt.legend(fontsize=18)\n",
    "plt.title(\"A figure showing a sigmoid function of one variable\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For this task you should:\n",
    "1. consider which features are suitable for scaling\n",
    "2. consider the z-scaling function: $f(x) = \\frac{x - \\mu}{\\sigma}$ where $\\mu=\\langle x \\rangle$ and $\\sigma^2= \\langle x^2 \\rangle - \\langle x \\rangle^2$. What impact does this function have on a normally distributed feature $x $~$N(\\mu, \\sigma^2)$? (you don't have to solve this by pen and paper - empirically showing by plotting is fine)\n",
    "3. Scale suitable features with a z-scaling function. Keep a copy of the unscaled data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "Classification models are formulated somewhat differently than regression models, but many of them actually optimize  the same problem and will have the same minimum. In that way Linear Regression solved by OLS serves as a natural bridge to the model we'll introduce, namely; Logistic Regression (Artificial Neural Networks can be used but will be introduced in a different project). A third model we'll briefly introduce, Random Forests, has a slightly different formalization, but this difference is very convenient in many cases as we'll get back to.\n",
    "\n",
    "Because of the problem of overfitting (using a more complex function to approximate a lower complexity phenomena) the data needs to be split in \"training\" and \"test\" sets. The sets are disjoint, which is to say that there is no common element to them. The training data is used to fit the model, and the test data is used to check the out-of-sample (OOS) generalization error, which measures how good we think the model will do on unseen \"real-world\" data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "We'll build up logistic regression (LR) by first considering a simpler model: the perceptron.\n",
    "The perceptron is a linear model on the form \n",
    "\n",
    "$s_i = \\mathbf{x}^T_i \\mathbf{w} + b $\n",
    "\n",
    "Where we cast this to a discrete output by the sign of $s_i$, i.e. $f(s_i) = sign(s_i)$. This obviously simple model was introduced earliy in the second half of the 20th century and was the foundation for neural networks today. In cases with noisy data the perceptron fails miserrably as it has no notion of how likely it is that a sample is in one class or the other. It then became preferable to have a \"soft\" classifier. Which is to say a classifier with output that can be treated as a probability. One such model is the logistic regression, which uses a sigmoid function to cast a unbounded input to a probability. The logistic sigmoid function which is the one such sigmoid we'll use is formulated as: \n",
    "\n",
    "$f(s) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "This function is obviously bounded as $f(s) \\in [0, 1]$, making it suitable to model a probability. It also has some nice properties that is useful in that it's monotonic and approaches a linear function at the origin. To see how it links to a a probability output we consider the odds which is simply a ratio of the probability of an event to a non-event. \n",
    "\n",
    "$o(p) = \\frac{p}{1-p}$\n",
    "\n",
    "The odds of an event is  a useful quantity, but hard to model as it is bounded in $\\mathcal{R}_+$ we instead consider the log-odds, or logit:\n",
    "\n",
    "$logit(p) = \\log{(o(p))}$\n",
    "\n",
    "Which is not bounded, making it convenient to model. Additionally it ties to the logistic function as a logit input to the sigmoid is the probability of that event. \n",
    "\n",
    "$f(\\log(o(p)) = \\frac{1}{1+e^{\\log{(o(p))}}} = p$\n",
    "\n",
    "The input to the logistic function the can be interpreted as log-odds, and we then formulate logistic regression as: \n",
    "\n",
    "$P(y_i = 1 |\\mathbf{x_i}; \\mathbf{w}, b) = \\frac{1}{1+e^{\\mathbf{x}_i^T\\mathbf{w} + b}}$\n",
    "\n",
    "The above equation reads as: \n",
    "* The probability that event $y_i$ happens. Where the event could for example that a student fails a class.\n",
    "* Given a datapoint, $\\mathbf{x_i}$ containing student information like GPA, attendance, gender etc.\n",
    "* And the model parameters $\\mathbf{w}$ which are constant with regard to changing the input and output, thus we separate it with a semicolon.\n",
    "\n",
    "This formulation means that we can interpret the coefficients of our model, $\\mathbf{w}$, when fit as relative log-odds for an event, giving us a nice interpretability of our model. The fitting procedure is outside the scope of this lecture, but can be found in the paper by Mehta et. al listed at the head of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: fitting models \n",
    "\n",
    "In this task you should: \n",
    "\n",
    "1. Using `scikit-learn` the data must be appropriately split in separate data-sets\n",
    "2. Fit a classification model from `scikit-learn` to the appropriate data (implementing this as a function is recommended)\n",
    "3. Evaluate this model using two different classification metrics. You should also plot the Reciever Operator Characteristic curve. What do they tell you? \n",
    "\n",
    "#### Extra:\n",
    "4. Evaluate whether the scaling impacts the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning\n",
    "\n",
    "There are many parameters that can impact the performance of your model. If they are not explicitly optimized we call them hyperparameters and making sure they have a good value is an active area of research still as the number of such parameters grow and the computational complexity of models grow with them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: hyperparameter search\n",
    "\n",
    "In this task you should: \n",
    "1. write your own function to do grid-search or random-search for two hyperparameters of your model. Consider the spacing for each parameter, should it be linear, logarithmic...? (hint: regularization strength is a good bet for most models).\n",
    "2. Plot or tabulate the results using a metric suited to this problem (hint: in `matplotlib.pyplot` there is a function`imshow` that can be very useful for this). \n",
    "\n",
    "#### Extra:\n",
    "3. Using `scikit-learn` add cross-validation to your hyperparameter search. Cross validation gives you a better estimate of your models performance, why? (see [this blog](https://towardsdatascience.com/cross-validation-70289113a072) for an explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis \n",
    "\n",
    "In classification, as in regression, the estimation of which features are _salient_ or important to the prediction is important to the analysis. The explicit procedure by which you can do this is model dependent, but for logistic regression the feature weights are often indicators of feature importance (think back to task 1a fpr statements that might modify this). One general way of measuring feature importance is by _recursive feature elimination_ which is just a fancy name for systematically picking out features you don't need by greedily checking how well your model does without it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Recursive Feature Elimination\n",
    "\n",
    "In this task you shoud:\n",
    "\n",
    "1. Use the provided code to perform feature extraction. The method included is model agnostic but suffers from performance issues. It's completely linear, it does a greedy search over all feature combinations leading to a long running time ( it goes as $\\mathcal{O}(2^N)$ ). What ways do you see of improving this algorithm? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import chain, combinations\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "\n",
    "def feature_selection(model, x_train, x_test, y_train, y_test, score=accuracy_score):\n",
    "    \"\"\"\n",
    "    arguments: \n",
    "    ----------\n",
    "    model: A model implementing .fit and .predict methods\n",
    "    x_train: a N-samples x features matrix of data to train on\n",
    "    y_train: targets to train against\n",
    "    x_test: hold out set to estimate OOS error\n",
    "    y_test: hold out targets to estimate OOS error\n",
    "    \n",
    "    kwargs:\n",
    "    score: a score function measuring model performance, must implement __call__(y_true, y_pred)\n",
    "    \n",
    "    returns:\n",
    "    which: index(indices) of the subset and performance corresponding to\n",
    "        the minimum within one std of max performance \n",
    "    model_performance: array of all scores as measured by score \n",
    "    subsets: list describing the powerset of the features\n",
    "    --------\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_features = x_train.shape[1]\n",
    "    feature_indices = np.arange(n_features)\n",
    "    \n",
    "    #generate a powerset [abc]->[a], [b], [c], [ab], [ac], [bc], [abc]\n",
    "    #to grab the columns from x for a given experiment\n",
    "    subset_iterator = chain.from_iterable(combinations(feature_indices, i) for i in range(1, len(feature_indices)+1))\n",
    "    model_performance = np.zeros(2**n_features)\n",
    "    subsets = [0]*(2**n_features)\n",
    "\n",
    "    for i, feature_subset in enumerate(subset_iterator):\n",
    "        #select subset data\n",
    "        train_subset = x_train[:, feature_subset]\n",
    "        test_subset = x_test[:, feature_subset]\n",
    "        #clone model with hyperparams but without weights to perform fitting\n",
    "        subset_model = sklearn.base.clone(model)\n",
    "        subset_model.fit(train_subset, y_train)\n",
    "        #evaluate performance\n",
    "        subset_performance = score(y_test, subset_model.predict(test_subset))\n",
    "        \n",
    "        #save configuration and performance\n",
    "        model_performance[i] = subset_performance\n",
    "        subsets[i] = feature_subset\n",
    "    \n",
    "    performance_std = model_performance.std()\n",
    "    best_performance = model_performance.max()\n",
    "    within_one_std = model_performance > (best_performance - performance_std)\n",
    "    min_within_one_std = model_performance[within_one_std].min()\n",
    "\n",
    "    which = np.where(model_performance == min_within_one_std)\n",
    "    return which, model_performance, subsets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
